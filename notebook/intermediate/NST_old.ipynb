{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytorch_libraries.model_NeuralStyleTransfer import *\n",
    "from IPython.display import clear_output\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "# For normalization, see https://github.com/pytorch/vision#models\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                                                     (0.229, 0.224, 0.225))\n",
    "                               ]\n",
    "                              )\n",
    "\n",
    "\n",
    "content = Variable(load_image('../../data/images/chicago2.jpg', transform ))\n",
    "style   = Variable(load_image('../../data/images/v1.jpg', transform, shape = [content.size(3), content.size(2)] ))\n",
    "\n",
    "image   = Variable(content.data.clone(), requires_grad=True)\n",
    "\n",
    "vgg     = VGGNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([image], lr=1e-2)\n",
    "STYLE_WEIGHT  = 1000.0\n",
    "TOTAL_OPTSTEP = 100\n",
    "\n",
    "before = image.clone().cpu().data.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/100], Content Loss: 16.6283, Style Loss: 159701.0156, change 0.0224\n",
      "Step [20/100], Content Loss: 22.8062, Style Loss: 68588.5391, change 0.0159\n",
      "Step [30/100], Content Loss: 26.6445, Style Loss: 42595.1445, change 0.0106\n",
      "Step [40/100], Content Loss: 27.5927, Style Loss: 29938.7266, change 0.0071\n",
      "Step [50/100], Content Loss: 28.9646, Style Loss: 22989.2988, change 0.0052\n",
      "Step [60/100], Content Loss: 29.7151, Style Loss: 18821.1777, change 0.0040\n",
      "Step [70/100], Content Loss: 30.5004, Style Loss: 16019.4629, change 0.0033\n",
      "Step [80/100], Content Loss: 30.9950, Style Loss: 13982.4746, change 0.0028\n",
      "Step [90/100], Content Loss: 31.4746, Style Loss: 12439.2998, change 0.0025\n",
      "Step [100/100], Content Loss: 31.9156, Style Loss: 11225.8037, change 0.0023\n"
     ]
    }
   ],
   "source": [
    "for step in range(TOTAL_OPTSTEP):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        image_features   = vgg(image)\n",
    "        content_features = vgg(content)\n",
    "        style_features   = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        \n",
    "        # for each layer\n",
    "        lw = [0,0,1,10,100]\n",
    "        for i, fi in enumerate(image_features):\n",
    "            fc = content_features[i]\n",
    "            fs = style_features[i]\n",
    "            \n",
    "            # Compute content loss (target and content image)\n",
    "            content_loss += torch.mean((fi - fc)**2)\n",
    "            \n",
    "            # Reshape conv features\n",
    "            _, c, h, w = fi.size()\n",
    "            fi = fi.view(c, h * w)\n",
    "            fs = fs.view(c, h * w)\n",
    "            # Compute gram matrix  \n",
    "            grami = torch.mm(fi, fi.t())\n",
    "            grams = torch.mm(fs, fs.t())\n",
    "            # Compute style loss (target and style image)\n",
    "            style_loss += torch.mean((grami - grams)**2) / (c * h * w) * STYLE_WEIGHT #* lw[i] / (c * h * w) \n",
    "            \n",
    "        # Compute total loss, backprop and optimize\n",
    "        #loss = content_loss + style_loss \n",
    "        loss =  style_loss \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                 \n",
    "        if (step+1) % 10 == 0:\n",
    "            after = image.clone().cpu().data.clamp_(0, 1)\n",
    "            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f, change %.4f' \n",
    "                   %(step+1, TOTAL_OPTSTEP, content_loss.data[0], style_loss.data[0], torch.mean(torch.abs(after-before))))\n",
    "            before = after\n",
    "    \n",
    "        if (step+1) % 10 == 0:\n",
    "            # Save the generated image\n",
    "            denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = image.clone().cpu().squeeze()\n",
    "            img = denorm(img.data).clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, '../../plot/nst/output-%d.png' %(step+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU (inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU (inplace)\n",
       "  (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU (inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU (inplace)\n",
       "  (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU (inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU (inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU (inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU (inplace)\n",
       "  (18): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU (inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU (inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU (inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU (inplace)\n",
       "  (27): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU (inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU (inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU (inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU (inplace)\n",
       "  (36): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
