{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytorch_libraries.model_NeuralStyleTransfer import *\n",
    "from IPython.display import clear_output\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "# For normalization, see https://github.com/pytorch/vision#models\n",
    "transform = transforms.Compose([transforms.Resize(300),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "\n",
    "content = Variable(load_image('../../data/images/chicago.jpg', transform ))\n",
    "style   = Variable(load_image('../../data/images/wave.jpg', transform, shape = [content.size(3), content.size(2)] ))\n",
    "\n",
    "target  = Variable(content.data.clone(), requires_grad=True)\n",
    "\n",
    "vgg     = VGGNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([target], lr=1e-2)\n",
    "STYLE_WEIGHT  = 1000.0\n",
    "TOTAL_OPTSTEP = 100\n",
    "\n",
    "before = target.clone().cpu().data.clamp_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/100], Content Loss: 8.9336, Style Loss: 27267.9531, change 0.0477\n",
      "Step [20/100], Content Loss: 11.7889, Style Loss: 9631.9404, change 0.0317\n",
      "Step [30/100], Content Loss: 12.6432, Style Loss: 6260.2690, change 0.0175\n",
      "Step [40/100], Content Loss: 12.9782, Style Loss: 4313.8877, change 0.0119\n",
      "Step [50/100], Content Loss: 13.2979, Style Loss: 3273.8140, change 0.0088\n",
      "Step [60/100], Content Loss: 13.5474, Style Loss: 2660.8677, change 0.0068\n",
      "Step [70/100], Content Loss: 13.7177, Style Loss: 2262.3555, change 0.0055\n",
      "Step [80/100], Content Loss: 13.8515, Style Loss: 1982.7922, change 0.0047\n",
      "Step [90/100], Content Loss: 13.9541, Style Loss: 1773.3322, change 0.0041\n",
      "Step [100/100], Content Loss: 14.0440, Style Loss: 1608.9413, change 0.0037\n"
     ]
    }
   ],
   "source": [
    "for step in range(TOTAL_OPTSTEP):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features  = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features   = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        \n",
    "        # for each layer\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            \n",
    "            # Compute content loss (target and content image)\n",
    "            content_loss += torch.mean((f1 - f2)**2) \n",
    "            # Reshape conv features\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "            # Compute gram matrix  \n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "            # Compute style loss (target and style image)\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) * STYLE_WEIGHT\n",
    "            \n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + style_loss \n",
    "        #loss =  style_loss \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                 \n",
    "        if (step+1) % 10 == 0:\n",
    "            after = target.clone().cpu().data.clamp_(0, 1)\n",
    "            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f, change %.4f' \n",
    "                   %(step+1, TOTAL_OPTSTEP, content_loss.data[0], style_loss.data[0], torch.mean(torch.abs(after-before))))\n",
    "            before = after\n",
    "    \n",
    "        if (step+1) % 10 == 0:\n",
    "            # Save the generated image\n",
    "            #denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().cpu().squeeze()\n",
    "            img = img.data.clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, '../../plot/nst/output-%d.png' %(step+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU (inplace)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU (inplace)\n",
       "  (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU (inplace)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU (inplace)\n",
       "  (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU (inplace)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU (inplace)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU (inplace)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU (inplace)\n",
       "  (18): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU (inplace)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU (inplace)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU (inplace)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU (inplace)\n",
       "  (27): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU (inplace)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU (inplace)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU (inplace)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU (inplace)\n",
       "  (36): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " models.vgg19(pretrained=True).features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
