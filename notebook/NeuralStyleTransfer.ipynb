{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytorch_libraries.model_NeuralStyleTransfer import *\n",
    "from IPython.display import clear_output\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "# For normalization, see https://github.com/pytorch/vision#models\n",
    "transform = transforms.Compose([transforms.Resize(250),\n",
    "                                transforms.ToTensor()])\n",
    "style   = Variable(load_image('../data/images/s3.jpg', transform))\n",
    "content = Variable(load_image('../data/images/00.jpg', transform))\n",
    "target  = Variable(content.data.clone(), requires_grad=True)\n",
    "\n",
    "vgg     = VGGNet()#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([target], lr=5e-3)\n",
    "    \n",
    "STYLE_WEIGHT = 1e5\n",
    "TOTAL_OPTSTEP= 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/300], Content Loss: 0.0309, Style Loss: 2.4085\n",
      "Step [20/300], Content Loss: 0.0513, Style Loss: 1.2712\n",
      "Step [30/300], Content Loss: 0.0585, Style Loss: 0.8615\n",
      "Step [40/300], Content Loss: 0.0616, Style Loss: 0.6306\n",
      "Step [50/300], Content Loss: 0.0635, Style Loss: 0.4933\n",
      "Step [60/300], Content Loss: 0.0651, Style Loss: 0.4039\n",
      "Step [70/300], Content Loss: 0.0665, Style Loss: 0.3410\n",
      "Step [80/300], Content Loss: 0.0679, Style Loss: 0.2945\n",
      "Step [90/300], Content Loss: 0.0691, Style Loss: 0.2585\n",
      "Step [100/300], Content Loss: 0.0702, Style Loss: 0.2299\n",
      "Step [110/300], Content Loss: 0.0712, Style Loss: 0.2068\n",
      "Step [120/300], Content Loss: 0.0722, Style Loss: 0.1878\n",
      "Step [130/300], Content Loss: 0.0731, Style Loss: 0.1718\n",
      "Step [140/300], Content Loss: 0.0739, Style Loss: 0.1582\n",
      "Step [150/300], Content Loss: 0.0746, Style Loss: 0.1465\n",
      "Step [160/300], Content Loss: 0.0753, Style Loss: 0.1364\n",
      "Step [170/300], Content Loss: 0.0760, Style Loss: 0.1275\n",
      "Step [180/300], Content Loss: 0.0766, Style Loss: 0.1197\n",
      "Step [190/300], Content Loss: 0.0772, Style Loss: 0.1127\n",
      "Step [200/300], Content Loss: 0.0778, Style Loss: 0.1065\n",
      "Step [210/300], Content Loss: 0.0783, Style Loss: 0.1010\n",
      "Step [220/300], Content Loss: 0.0788, Style Loss: 0.0960\n",
      "Step [230/300], Content Loss: 0.0793, Style Loss: 0.0915\n",
      "Step [240/300], Content Loss: 0.0797, Style Loss: 0.0874\n",
      "Step [250/300], Content Loss: 0.0802, Style Loss: 0.0837\n",
      "Step [260/300], Content Loss: 0.0806, Style Loss: 0.0803\n",
      "Step [270/300], Content Loss: 0.0810, Style Loss: 0.0772\n",
      "Step [280/300], Content Loss: 0.0814, Style Loss: 0.0744\n",
      "Step [290/300], Content Loss: 0.0818, Style Loss: 0.0719\n",
      "Step [300/300], Content Loss: 0.0822, Style Loss: 0.0695\n"
     ]
    }
   ],
   "source": [
    " for step in range(TOTAL_OPTSTEP):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features  = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features   = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        \n",
    "        decay = 1\n",
    "        # for each layer\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            \n",
    "            # Compute content loss (target and content image)\n",
    "            content_loss += torch.mean((f1 - f2)**2) * decay\n",
    "            decay *= 0.1\n",
    "\n",
    "            # Reshape conv features\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "            # Compute gram matrix  \n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "            # Compute style loss (target and style image)\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "\n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + STYLE_WEIGHT * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % 10 == 0:\n",
    "            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f' \n",
    "                   %(step+1, TOTAL_OPTSTEP, content_loss.data[0], style_loss.data[0]))\n",
    "    \n",
    "        if (step+1) % 10 == 0:\n",
    "            # Save the generated image\n",
    "            #denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().cpu().squeeze()\n",
    "            img = img.data.clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-%d.png' %(step+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
