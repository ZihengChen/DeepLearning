{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pytorch_libraries.model_NeuralStyleTransfer import *\n",
    "from IPython.display import clear_output\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "# For normalization, see https://github.com/pytorch/vision#models\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.ToTensor()])\n",
    "style   = Variable(load_image('../data/images/s2.jpg', transform))\n",
    "content = Variable(load_image('../data/images/c2.jpg', transform))\n",
    "target  = Variable(content.data.clone(), requires_grad=True)\n",
    "\n",
    "vgg     = VGGNet().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([target], lr=5e-3)\n",
    "    \n",
    "STYLE_WEIGHT = 1e4\n",
    "TOTAL_OPTSTEP= 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/100], Content Loss: 10.0895, Style Loss: 0.0659\n",
      "Step [20/100], Content Loss: 10.2163, Style Loss: 0.0452\n",
      "Step [30/100], Content Loss: 10.1798, Style Loss: 0.0380\n",
      "Step [40/100], Content Loss: 10.1741, Style Loss: 0.0352\n",
      "Step [50/100], Content Loss: 10.1874, Style Loss: 0.0337\n",
      "Step [60/100], Content Loss: 10.2080, Style Loss: 0.0326\n"
     ]
    }
   ],
   "source": [
    " for step in range(TOTAL_OPTSTEP):\n",
    "        \n",
    "        # Extract multiple(5) conv feature vectors\n",
    "        target_features  = vgg(target)\n",
    "        content_features = vgg(content)\n",
    "        style_features   = vgg(style)\n",
    "\n",
    "        style_loss = 0\n",
    "        content_loss = 0\n",
    "        \n",
    "        # for each layer\n",
    "        for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "            # Compute content loss (target and content image)\n",
    "            content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "            # Reshape conv features\n",
    "            _, c, h, w = f1.size()\n",
    "            f1 = f1.view(c, h * w)\n",
    "            f3 = f3.view(c, h * w)\n",
    "            # Compute gram matrix  \n",
    "            f1 = torch.mm(f1, f1.t())\n",
    "            f3 = torch.mm(f3, f3.t())\n",
    "            # Compute style loss (target and style image)\n",
    "            style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "\n",
    "        # Compute total loss, backprop and optimize\n",
    "        loss = content_loss + STYLE_WEIGHT * style_loss \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step+1) % 10 == 0:\n",
    "            print ('Step [%d/%d], Content Loss: %.4f, Style Loss: %.4f' \n",
    "                   %(step+1, TOTAL_OPTSTEP, content_loss.data[0], style_loss.data[0]))\n",
    "    \n",
    "        if (step+1) % 10 == 0:\n",
    "            # Save the generated image\n",
    "            #denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "            img = target.clone().cpu().squeeze()\n",
    "            img = img.data.clamp_(0, 1)\n",
    "            torchvision.utils.save_image(img, 'output-%d.png' %(step+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
